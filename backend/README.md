# Scientific Document Q&A Backend

This backend powers the Scientific Document Q&A System, enabling local, privacy-preserving document analysis and question answering using open-source Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG).

---

## ğŸš€ Overview
- **Framework:** Python + Flask
- **Core Features:**
  - Upload and process documents (PDF, DOCX, TXT, HTML, CSV, JSON)
  - Chunking, summarization, and vector storage (ChromaDB)
  - Hybrid semantic/keyword search for retrieval
  - LLM-powered Q&A and summarization (HuggingFace, transformers, llama.cpp)
  - All processing is localâ€”your data never leaves your machine

---

## ğŸ“ Folder Structure

```
backend/
â”œâ”€â”€ app.py                # Main Flask app and API endpoints
â”œâ”€â”€ config.py             # Centralized configuration (models, paths, settings)
â”œâ”€â”€ document_processor.py # Handles file upload, chunking, and caching
â”œâ”€â”€ llm_manager.py        # Manages LLM inference (summarization, Q&A)
â”œâ”€â”€ rag_pipeline.py       # Orchestrates RAG workflow (retrieval, reranking, answer)
â”œâ”€â”€ vector_store.py       # Vector DB (ChromaDB) and embedding management
â”œâ”€â”€ utils.py              # Utility functions (logging, ID generation, etc.)
â”œâ”€â”€ models/               # (Optional) Local model files
â”œâ”€â”€ uploads/              # Uploaded documents
â”œâ”€â”€ vector_db/            # ChromaDB persistent storage
â”œâ”€â”€ cache/                # Chunk and embedding cache
â”œâ”€â”€ logs/                 # Log files
```

---

## âš™ï¸ Configuration
- All settings (model, device, chunk size, etc.) are in `config.py` and can be overridden by environment variables.
- Default LLM: `mistralai/Mistral-7B-Instruct-v0.3` (HuggingFace)
- Default embeddings: `sentence-transformers/all-MiniLM-L6-v2`
- Device: CPU, CUDA, or MPS (auto-detect)

---

## ğŸ› ï¸ How to Run

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   # (You may need additional system packages for some document types)
   ```

2. **(Optional) Set environment variables:**
   - `DEFAULT_MODEL`, `EMBEDDINGS_MODEL`, `DEVICE`, etc.

3. **Start the backend:**
   ```bash
   python app.py
   # By default runs on http://localhost:8000
   ```

---

## ğŸ”— API Endpoints

- `GET  /api/health` â€” Health check and system info
- `GET  /api/documents` â€” List all processed documents
- `POST /api/upload` â€” Upload a document (file or text)
- `GET  /api/documents/<doc_id>` â€” Get document details
- `DELETE /api/documents/<doc_id>` â€” Delete a document
- `POST /api/answer` â€” Ask a question about a document
- `GET  /api/document/<doc_id>/download` â€” Download chunk summaries

### Example: Upload a Document (cURL)
```bash
curl -F "file=@yourfile.pdf" http://localhost:8001/api/upload
```

### Example: Ask a Question
```bash
curl -X POST http://localhost:8001/api/answer \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the summary?", "document_id": "<doc_id>"}'
```


---

## ğŸ Troubleshooting
- Check `logs/` for error logs
- Ensure all dependencies are installed (see `requirements.txt`)
- For GPU, set `DEVICE=cuda` and ensure CUDA drivers are installed
- For HuggingFace API, set `HF_API_TOKEN` in your ---

## ğŸ§© Main Components
- **DocumentProcessor:** Loads, chunks, and caches documents
- **VectorStore:** Stores and retrieves embeddings (ChromaDB)
- **RAGPipeline:** Retrieval, reranking, and answer generation
- **LLMManager:** Manages LLM inference (summarization, Q&A)

## ğŸ“š References
- [Flask](https://flask.palletsprojects.com/)
- [ChromaDB](https://www.trychroma.com/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/index)
- [LangChain](https://python.langchain.com/)

---

**All processing is local and private. Your data never leaves your machine.** 